\chapter{Hacking Traffic Bursts: An Introduction} \label{chap:chap-1}

% if you want a short header you can use the following command
% \chapter[short-header-name]{chapter-title} \label{chap:chap-1}

Measurement studies show that traffic is bursty across a wide range of timescales in diverse contexts such as Ethernet LANs \cite{selfsim}, WANs \cite{wan}, data centers \cite{fbIMC22}, and WWW traffic \cite{web}.
In particular, microsecond-scale congestion events, sometimes called \emph{microbursts}, have been the focus of numerous measurement and control papers recently \cite{swift, conquest, snappy, radar, high-resolution, wild, fbIMC22,incast}. However, the modulating effect of host networking on traffic burstiness at various timescales is relatively less investigated. This study starts by addressing this gap.
We ask \emph{what causes the traffic to emerge from hosts in bursts?} Is burstiness an \emph{scale-invariant} property of traffic, i.e., does the traffic retain its burstiness across a wide range of timescales, or do the microbursts become smooth at coarse timescales? Are canonical burst countermeasures such as TCP pacing and packet scheduling effective in curtailing bursts?

These questions have far-reaching implications for network performance and design. Controlling bursts at different timescales requires deploying mechanisms that operate at the corresponding pace. Microbursts, for instance, require real-time techniques with sub-RTT control loops, whereas bursts at longer timescales can be more effectively managed by resource provisioning techniques such as topology engineering and routing that take seconds to minutes to complete \cite{gemini}.

Unfortunately, studying the impact of host networking on bursts is complex. Take the Linux network stack as an example: the egress traffic that originates from the Linux kernel stack passes through many layers and optimizations before arriving at the wire. Transport protocol internals like initial window size, cumulative acknowledgments, queueing disciplines (qdiscs), driver rings, segmentation offloading, and hardware packet scheduler at the NIC all handle the traffic.
All these elements and their complex interactions can play a role in forming or suppressing bursts at various timescales. 
%
These challenges are further compounded by the heterogeneity, scale, and 
the velocity of evolution in today's networks that constantly change in response to increasing demand and the rollout of new services \cite{snap, evolvable, govindan2016evolve}. 

\section{Contribution I: Measuring the Burstiness}

We build \textit{Valinor} \cite{valinor}, a high-resolution traffic measurement framework that enables network operators to systematically and periodically dissect the elements of host networking, their impact on traffic burstiness in isolation, and importantly, their interactions with the emergent traffic patterns, all at different timescales. To ensure visibility into the impact of the software stack and the shape of the traffic on the wire through time, \textit{Valinor} is composed of two main components: 1) An in-host timestamping framework (\textit{Valinor-H}) based on \textit{eBPF} that collects egress packet metadata nearly at the last stage of software stack processing. 2) An in-network packet timestamping framework (\textit{Valinor-N}) that captures packet arrival timestamps in the programmable switch data plane immediately after the NIC, and sends the timestamp data to offline servers for collection, storage, and burstiness analysis. 

Our analysis of the impact of host networking on the shape of traffic using \textit{Valinor} reveals some surprising results. 
% different contexts
As an example, classical work paints a unifying and consistent picture of scale-invariant burstiness, i.e, they show the same degree of variability across a wide range of timescales in a variety of different network types \cite{selfsim, wan, web}.
% 2: app-layer
It has also been established that this scale-invariant burstiness is primarily caused by \emph{application layer} characteristics such as long-tailed flow size distributions and is ``robust'': it holds for a variety of transport protocols (e.g., TCP Reno, Vegas, and flow controlled UDP) and various network configurations \cite{filesize, feldmann1999dynamics}.

In contrast, our investigations paint a more nuanced and complex picture. We show that burstiness at various timescales varies significantly across host configurations (hardware configurations, transport protocols, scheduling, etc.). We also show the pronounced modulating effect of below application layer elements on bursts. This implies that, for the same heavy-tailed flow size distribution, the ultimate shape of traffic on the wire depends heavily on the host configuration such as the NIC scheduler. Plus, \textit{Valinor}'s analysis of newer reliable transport protocols (e.g., Homa \cite{homa}, DCTCP \cite{dctcp}, and BBR \cite{bbr}) reveals the high degree of variability of burstiness for these protocols. As an example, BBR is less bursty not just at fine timescales (a result that is consistent with the literature \cite{homa, homa-impl}) but also at coarse timescales. The latter finding (new to the best of our knowledge) implies that techniques such as topology engineering \cite{gemini} and multi-timescale congestion control \cite{mts_cc}---premised on the long-range burstiness of traffic---may yield limited performance improvements under these new protocols. 

Finally, given the impact of some variants of transport protocols on bursts, we quantify the effectiveness of TCP pacing and active queue management paradigms such as CoDel \cite{codel} in qdiscs (software packet schedulers) in mitigating bursts. 
%
Our results show the pronounced impact of lower-layer functions (residing in the driver and NIC) on forming the ultimate shape of traffic on the wire relative to the higher-layer software operations of the TCP/IP stack and qdiscs. 
As an example, active queue management techniques such as CoDel and RED in the Linux kernel try to prevent the formation of large and lasting bursts. However, our results show that their impact is effectively erased by offloading (TCP Segmentation Offloading (TSO), serialization, etc.) and the NIC scheduler. 
For example, while in isolation, the frequency of large 300 kB bursts under CoDel is 500 times lower than FIFO, this difference is barely visible on the wire after packets pass through the multi-queue NIC with segmentation offloading.
Moreover, TCP pacing enforced in the qdiscs generates between 1.8$\times$-19$\times$ larger bursts when NIC scheduler and offloading are in action compared to when in isolation.\footnote{Despite making the traffic bursty and hard to manage, these low-level functions are essential for reducing the processing overhead and meeting the increasingly high link rates. For example, disabling TCP segmentation offload results in a 3$\times$ increase in CPU utilization, 71\% lower throughput, and a 46\% increase in median packet RTTs for a multi-flow \textit{Iperf} test. 
Relatedly, disabling MQ results in a 4\% decline in the throughput of the same workload.
} 
This result indicates that the countermeasures for controlling bursts should be moved further down the packet processing pipeline at the end hosts. 

Our results on the variability of burstiness (based on hardware configurations, transports, etc.)---combined with the ever-evolving workloads and features in today's networks---highlight the need for periodic traffic measurement and analysis. 
In chapter \ref{chap:chapter-2}, we introduce the mathematical notions developed for capturing bursts across time, present their practical implications in networks (\S\ref{sec:valinor-background}), provide some background on host networking and the design space of burst measurement frameworks (\S\ref{sec:valinor-space}), and present the design of Valinor (\S\ref{sec:valinor}) before delving into our findings (\S\ref{sec:valinor-findings}).

% ========================================
\section{Contribution II: Revisiting Fair Packet Scheduling}

The Internet is continuously evolving as a multi-party environment. To meet service performance requirements, enforcing isolation at different levels ranging from tenants to applications and individual flows is crucial \cite{sharing,loom,carousel,fairnic,mrfq,heracles}. 
% To this end, cloud providers and network operators employ a variety of solutions, including packet scheduling, to ensure that different workloads meet their quality of service demands within their resource utilization budget \cite{arachne,snap,ghost,syrup,caladan,drr,loom,aifo}.
Packet scheduling offers an opportunity to enforce fine-grained
control over network bandwidth both at the edge and core of the
network. Despite the abundance of scheduling techniques that rely on explicit labeling, such as strict priority queues \cite{aifo,karuna,afq} or time wheels \cite{carousel,calendar}, obtaining flow priorities or timestamps from
Internet transports is usually not feasible.
% and managing QoS across the Internet is an open problem.
Instead, Fair Queuing provides fine-grained fairness without flow information, and has been deeply studied from fairness, latency bounds, and computational complexity standpoints
\cite{gps,wfq,nagle,scfq,stfq,wf2q}. Since Fair Queuing schedulers suffer from non-linear computational complexity which hinders their deployment \cite{stfq,scfq,wf2q}, a middle-ground alternative is to
use Deficit Round-Robin (DRR) scheduling 
% or one of its derivatives 
because of its low overhead and scalability \cite{drr,aliquem,strr,frr}. Indeed, DRR is used everywhere from hardware traffic
management pipelines to software middleboxes, host stacks, and network interface cards 
\cite{loom,cisco,juniper,tc,pifo,intel710,intel810}.

Regrettably, the nature of the Internet traffic since Deficit Round-Robin was proposed has changed, and the drawbacks of DRR are increasingly problematic.
Workloads such as realtime video conferencing and voice communication, streaming, cloud gaming, and instant messaging have become widespread among more traditional web and file transfer \cite{rtc,pandemic,video,zoom,confucius,gaming,enterprise,baidu,canyousee}. Such workloads feature a diverse set of properties concerning flow counts, flow arrival rates, flow sizes, packet lengths, packet inter-arrivals, and burstiness \cite{cachelib,enterprise,milisampler,baidu,wild,autosens,canyousee}.
First, concerning burstiness, Internet traffic traces feature bursts of packets with diverse lengths and inter-arrivals \cite{caida, mawi,ccdc,wild,im}. Long bursts are more prone to create backlogged queues at the packet scheduler, while, broadly, short bursts belonging to latency-sensitive applications such as web and streaming  \cite{edge,passive} exhibit periodic on-off arrivals. For example, the
median response size for web workloads does not exceed 10 kB, while
Variable Bit-Rate (VBR) streaming workloads such as Zoom have
a median frame length of 10 kB with large idle gaps between frames
\cite{passive}.

Our experiments with DRR reveal that under
a backlogged scheduler, a bursty latency-sensitive flow has to wait
for up to a full scheduling round, i.e., until all the flows have been
serviced at least once, to be able to send its traffic. This can be
detrimental to the performance of such flows, resulting in much higher application latency compared to Fair Queueing Schedulers. 
Even with recent enhancements to DRR such as Sparse Flow Optimization (SFO) \cite{sfo}, which prioritize new incoming flows from backlogged flows to improve responsiveness, bursts with more than one packet are prone to being quickly demoted to lower-priority and having to compete with backlogged flows, thus experiencing long delays.

Secondly, packet lengths are diverse and skewed \cite{caida,
mawi,ccdc,wild,im,harmony} and packet length distribution for
wide-area network traffic is difficult to generalize. Many modern
applications use small packets to reduce latency and the impact of
packet loss \cite{webrtc-teams, zoom-quality}. Network Interface
hardware acceleration \cite{linux-gso-gro} may produce very large
packets (64 kB).
DRR relies on a fixed, user-specified quantum equal to (or higher
than) the maximum packet size in the network \cite{drr}, making it
hard to tune. Configuring DRR with a conservatively large quantum can
be detrimental to  intermediate buffers as it allows the scheduler
to send large bursts of packets when servicing each flow
\cite{strr,aliquem}, ultimately causing increased response times for
latency-sensitive applications.
\textit{No packet scheduler today can offer low resource utilization,
low latency, and adaptability to packet sizes without explicit flow information feedback.}

Our work makes several novel contributions to the fair scheduling domain.
First, we show that the drawbacks of DRR, i.e., high latency for bursty flows and
vulnerability to quantum choice, are measurable at the application
level, even on high speed networks. We further identify the
trade-off between latency and CPU utilization as an inherent part of
DRR's design and argue that there is no correct answer to configuring
DRR (\S\ref{sec:scrr-background}).

Second, we formally outline the design of Self-Clocked Round-Robin
(\textit{SCRR}) \cite{scrr}, a novel multi-queue round robin scheduler. \textit{SCRR}
borrows virtual clocking from Fair Queuing \cite{stfq,scfq} to eliminate the quantum and enforce fairness, and services sub-queues like DRR in
strict sequence for simplicity and scalability. As a result, \textit{SCRR} is
parameter-less, light, scalable, and adapts to both bursts and
packet length variations in the network. We design various \textit{SCRR}
enhancements to efficiently prioritize bursty flows, in order to
reduce the application latency.
When an idle  flow becomes active, the scheduler must decide its bandwidth allowance through its virtual clock setting. 
Backed by strong theoretical virtual clocking principles, \textit{SCRR} can efficiently adjust the virtual clocks for short, bursty flows to allow them to quickly make progress without violating the scheduler's fairness bounds (\S\ref{sec:scrr-design}).
Third, we provide rigorous proof of flow burstiness and fairness for
\textit{SCRR}. Precise virtual time accounting guarantees
that \textit{SCRR} has the same fairness as DRR with a low maximum burstiness bound (\S\ref{sec:scrr-analysis}).

Fourth, we evaluate \textit{SCRR} and competing schedulers on a physical testbed under Cubic and BBRv3 
using novel experiments. Application-level
fairness is tested with thousands of active TCP flows while application latency is measured for request-response flows and VBR traffic.
Our results demonstrate that employing \textit{SCRR} as a software switch
results in 23\% lower CPU utilization compared to DRR with 1500B
quantum. Our results further reveal that \textit{SCRR} offers 93$\times$, 71\%,
and 45\% lower response times over tail-drop, DRR with a small
quantum, and DRR with a large quantum,
respectively. (\S\ref{sec:scrr-evaluation})

\textit{SCRR} is a step toward more dynamic packet scheduling for multi-party
wide-area deployments. \textit{SCRR} is the first true alternative to DRR, it
offers the low latency of Start Time Fair Queuing with the low
complexity of DRR. Our ultimate goal is for \textit{SCRR} to replace existing
hardware and software packet schedulers because of its plug-and-play
deployment, low resource consumption, low scheduling latency, 
desirable fairness, and compatibility with bursty workloads.

% ========================================
\section{Contribution III: In-situ Reactions to Bursts}

% Driven primarily by two trends---the disaggregation of storage, compute, and memory across the network for cost savings and the rising demand for high-speed transmission in new technologies and applications \cite{swift, hpcc}---datacenters today have exceedingly stringent low-latency requirements. To enable resource disaggregation, remote resources (GPU, memory, disk, etc.) should be accessible over the network within 3-5$\mu s$ \cite{hpcc}. In emerging applications and technologies such as NVMe (non-volatile memory express) and large-scale machine learning workloads, the network is frequently the performance bottleneck because their storage and computation resources are extremely fast \cite{hpcc}. Despite significant progress towards building ultra-low-latency datacenter networks in recent years, why does the network continue to remain the performance bottleneck? A key challenge is reportedly \emph{microbursts}, short-lived periods of congestion that last for less than a millisecond and cause the majority of packet loss in datacenters \cite{wild, high-resolution, hpcc}. The extreme packet drops caused by \bursts \ lead to re-transmissions that impose significant latency and degrade application performance. 
% People talking about mbursts are citing random papers: From DCTCP to TLP to the recent ToN paper
%
Managing \bursts \ is challenging because of their short lifespans and their diverse and ever-changing root causes (applications, TCP artifacts such as ACK compression, offloading features in NICs, \etc \ \cite{high-resolution, bullet, measuring, dctcp, why, notes,ipbursts, coalescing, snap,valinor}). 
%Even measuring \bursts \ poses technical challenges. 
Rack-level traffic measurements at Facebook, for instance, show that more than 70\% of \bursts \ last for less than a few tens of microseconds, significantly shorter than the frequency of most deployed measurement frameworks \cite{high-resolution}. More recent studies confirm the prevalence of incast traffic patterns, where a large number of hosts simultaneously transmit data to a single receiver, in large-scale datacenters \cite{swift,milisampler,incast}.

% numerous studies have tried, at the edge and in the core. A large number of recent proposals try to manage \bursts \ at hosts and in the network core. 
Given the necessity of reacting to \bursts \ in situ and in real-time, a group of proposals attempts to manage \bursts \ in the network core, \eg via  balancing the load among multiple shortest paths \cite{drill}, deflecting the excess load across neighboring switches with spare capacity \cite{dibs, pabo}, and provisioning enough buffer space in switches to absorb bursts \cite{bufferFacebook}. Although effective for small scales and lightly loaded networks, network-centric techniques fail under load and at scale. Load balancing and buffer sizing techniques, for example, cannot manage large-scale incasts because the intensity of the burst in such cases exceeds the buffer capacity of any single datacenter switch (commonly shallow buffered) and the number of paths that the burst can be distributed among. Similarly, deflecting packets extends their path lengths, \eg by 20\% under 50\% offered load ($\S$\ref{sec:deflection}), and increases utilization which exacerbates the congestion if the network is already overloaded. By preventing packet drops, deflection further delays informing the hosts that they need to throttle their send rate. Our experiments show that under 80\% load, random deflection completes $10\times$ fewer incast queries and the average flow completion times is $45\%$ higher than a simple baseline. 
%Plus, packet deflection leads to excessive packet reordering that is detrimental to transport protocols and applications. 
Datacenters today experience a wide range of workloads, scale, and utilization, including frequent epochs of high utilization and extreme-scale incast events (thousands of flows arriving simultaneously at the same destination) \cite{nature, swift, hpcc}. For a burst management technique to be viable, it is an essential necessity that it handles extreme load gracefully.

On the other hand, given the greater visibility and control over the sources of traffic at the edge, another group of host-centric designs strives to proactively identify and prevent the formation of \bursts \ at the edge, sometimes with some feedback from the network (\eg regarding the queueing delay \cite{hpcc} and congestion \cite{dctcp}). Adding jitter in the application layer to prevent synchrony \cite{jitter}, credit-based transport protocols that coordinate and schedule the flows sent to a receiver \cite{pHost, NDP, homa}, and feedback-based congestion control protocols that strive to impede bursts and their subsequent packet loss by reacting to increasing RTTs and network congestion faster \cite{dctcp, SECN, hpcc, swift} are all examples of designs in which the hosts play a central role in preventing the formation of \bursts. Compared to core-centric designs, host-based techniques are fundamentally limited by their slower reaction to \bursts, typically an RTT or slower.
%

We advocate co-designing the networking layer in the core and the edge to make it burst-tolerant. Given that \bursts \ are extremely short-lived, the network core should be capable of handling them in real-time and in place, \eg by distributing microburst packets across the network. However, to remain efficient under various degrees of load, the network should distinguish between \bursts \ and an overall high degree of load and treat each differently. For example, while \emph{deflecting} the packets of a local, transient burst to other switches when the overall utilization is low improves flow completion times, \emph{dropping} them, when the overall load is high, helps reduce the congestion and improves performance. Making such distinctions can be greatly facilitated with some assistance from the edge. We show that a simple and efficient extension to senders' networking stack enables such discretion in the network core by tagging packets with flow size information. Similarly, on the receiver-side, a re-sequencing shim layer can retrieve the correct ordering of packets and thus shield the transport and application protocols from the excessive reordering caused by deflection.

To realize this vision, we design \textit{Vertigo} \cite{vertigo}, an edge-core co-design of the networking layer that leverages end-host knowledge of the flow size information to selectively deflect and drop packets based on their likely contribution to persistent congestion (as opposed to \bursts). \textit{Vertigo} consists of three components deployed on the path of a datacenter packet. (1) We design an extension to the end-hosts network stack that tracks and tags every packet inside a flow with the remaining bytes of its flow, referred to as RFS (Remaining Flow Size)\footnote{
In \S\ref{sec:eval}, we show that \textit{Vertigo} continues to be more effective than other baselines such as ECMP and DIBS \cite{dibs} even when flow size information is not available in advance. However, having this information improves its average incast query completion time by 13\%.
% In \S\ref{sec:eval}, we show that Vertigo continues to be more effective than the state-of-the-art such as DRILL \cite{drill} even when RFS is not available in advance. However, having this information improves its performance by 12\%.
}. A boosting module decreases the RFS fields of re-transmitted packets to ensure they do not starve. 
%Combined with mechanisms to detect packet re-transmissions, Vertigo is able to improve the completion of bursty flows. 
 (2) In the network core, when a switch receives a packet and the output queue of the packet is full, the switch selects the packet with the largest RFS among the newly arrived packet and the packets in its output queue to deflect:
%\ie to detour to a neighboring switch that is not necessarily on a packet's shortest path towards its destination. To deflect this packet, t
the switch then randomly selects two queues and inserts the packet into
% to 
the least loaded one. If both queues are full, the switch randomly selects one of them and drops the packet with the largest RFS among the enqueued packets and the deflected packet in order to keep the packets with the lowest RFS in the queue. (3) Finally, we design a transport-independent packet ordering framework on the receiver hosts' RX path that detects and buffers out-of-order packets to wait for packets experiencing prolonged RTTs due to deflection.
%. the intuition behind distinguishing mbursts and lasting congestion
Packets of the flows with more bytes 
% of
to send (\ie those with large RFS) are more likely to contribute to persistent congestion. When facing \bursts, Vertigo prioritizes such packets for deflecting (when a randomly selected queue in the switch has spare capacity) and for dropping (when the network is congested, indicated by two randomly selected queues being \emph{both} full at the same time). This enables \textit{Vertigo} to gracefully handle \bursts\  even under extreme loads.

\textit{Vertigo} is a burst-tolerant, fast L2/L3 routing technique for datacenters. Albeit more efficient than its L2/L3 counterparts, it still only provides a best-effort reachability service that may drop and reorder packets. Thus, it should be deployed below the appropriate transport protocols that provide higher-level services such as congestion control (\eg to throttle the send rate when the overall send rate is higher than the capacity of the network), loss recovery (due to congestion and failures), and fairness. In our evaluations, we test the performance of \textit{Vertigo} as the substrate running below TCP, DCTCP, and a state-of-the-art datacenter congestion control algorithm, Swift \cite{swift}. \textit{Vertigo} consistently outperforms the other L2/L3 baselines such as DIBS \cite{dibs} (a representative of deflection routing in datacenters), DRILL \cite{drill} (a microburst-tolerant load balancer), and ECMP (the most widely deployed forwarding protocol in datacenters) for all these transport protocols, especially under load and extreme incast scales. For example, in a network with 55\% overall link utilization and a bursty workload, \textit{Vertigo}+DCTCP reduces mean incast query completion times (QCT) by 48\%, and 58\% over DRILL+DCTCP and DIBS+DCTCP, respectively. Under 85\% load, incast queries finish 47\% and 68\% faster under \textit{Vertigo}+DCTCP compared to DRILL+DCTCP and DIBS+DCTCP. Under the same load (85\%), \textit{Vertigo}+Swift improves the QCT of DRILL+Swift and DIBS+Swift by a factor of 32 and 15, respectively.

At its core, \textit{Vertigo} is a deflection routing technique that harnesses hosts' visibility into the workload to remain efficient under extreme scales and loads. We demonstrate why a naive implementation of deflection quickly fails in typical datacenter scenarios (\S\ref{sec:deflection}), outline our design for making deflection practical in datacenters (\S\ref{sec:v2}), and evaluate the effectiveness of Vertigo in typical datacenter scenarios, including under various degrees of load, traffic burstiness, and two datacenter topologies, via extensive simulations and microbenchmarks (\S\ref{sec:eval}).

This thesis scratches the surface of burstiness, its definitions, implications, and design considerations.
We reiterate the importance of studying the network traffic burstiness at different timescales. We design and implement \textit{Valinor} a high-resolution measurement framework using programmable network capabilities in host network stacks and the switching fabric and identified the inefficacy of software-based measures inside the hosts in preventing burstiness. Our results further call for rethinking the design of existing traffic shaping mechanisms such as packet schedulers with traffic characteristics in mind. To this end, we target the Deficit Round-Robin (DRR) scheduler, a most popular fair packet scheduling paradigm for both software and hardware deployments, and identify several assumptions in DRR's design that remain incompatible with the dynamic and bursty nature of the Internet traffic today. We then propose Self-Clocked Round-Robin (SCRR), a novel scheduling paradigm for low-latency, high-throughput, and low burstiness. Finally, to address short-term bursts that overwhelm network buffers, we introduce selective packet deflection, a novel realtime reaction mechanism to packet loss in datacenter networks.

These contributions, when combined together, help realize burst-tolerant networks. However, the process of designing a fully burst-tolerant network would outlive this study. Each individual networking component is susceptible in creating bursts or performing poorly when facing bursty traffic. Moreover, the workload dynamics in both local networks and wide-area networks constantly change. Therefore, periodic measurements is a must to ensure that our networks are resilient against bursts. Finally, while packet deflection proves a strong solution to recover from the destructive impact of microbursts, more work is required to make it practical, efficient, and compatible with existing deployments (\S\ref{chap:chap-6}).